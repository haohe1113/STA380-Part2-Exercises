---
title: "Q5_Author_Attribution"
author: "Hao_He"
date: "8/15/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list=ls())
set.seed(1)
library(tm) 
library(tidyverse)
library(slam)
library(proxy)
library(caret)

#the readerplain function
readerPlain = function(fname){
  #convert a txt file to a PlainTextDocument
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
```

#Set up train and test data

*To start with, we created a process to transform the raw .txt file folders into TF-IDF matrices which we could run PCA on.*
*First, we used a for loop to put all .txt files into a corpus and their corresponding authors into a separated author list (as outcome). Then we did tokenization on the corpus and obtained the DTM and TF-IDF matrix from there.*
*We applied this process to C50Train and C50Test separately. We ended up having two TF-IDF matrices from the two folders.*

## Train data

### Create the train corpus which contains all the articles from train and a list of their author names

```{r}

#a list of links to authors' folders(folders each contain 50 articles for its author)
author_folder = Sys.glob('~/Desktop/Group_Project/data/ReutersC50/C50train/*') 

#article file link list and author list
arti_list = c() #a list of all article docs (50 articles x 50 authors)
author_list_tr = c() # a list of author names corresponding every doc in the corpus

for (author in author_folder) {
  name = substring(author, first = 59) #slice the folder link to obtain the name of the author
  articles = Sys.glob(paste0(author, '/*.txt')) #a list of all 50 articles from one author
  arti_list = append(arti_list, articles)
  author_list_tr = append(author_list_tr, rep(name, times = length(articles)))
}

#use the link list to create a list of readPlaindocs
c50_tr = lapply(arti_list, readerPlain)

#clean the doc names and apply them to c50_tr
mynames = arti_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
names(c50_tr) = mynames

#create the train corpus
cps_tr_raw = Corpus(VectorSource(c50_tr))
```

### Tokenization on train corpus
```{r}
cps_tr = cps_tr_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace)) %>%      # remove excess white-space
  tm_map(content_transformer(removeWords), 
         stopwords("en"))                               #remoce stop words

cps_tr
```

### create the DTM and TF-IDF matrix for train corpus

```{r}
#the DTM matrix
DTM_tr = DocumentTermMatrix(cps_tr) %>%
  removeSparseTerms(.99) #remove sparse elements which don't show in 95% of the articles

#the TF-IDF matrix
tfidf_tr = as.matrix(weightTfIdf(DTM_tr))
```


## Test data

### Create the test corpus which contains all the articles from test and a list of their author names

```{r}

#a list of links to authors' folders(folders each contain 50 articles for its author)
author_folder = Sys.glob('~/Desktop/Group_Project/data/ReutersC50/C50test/*')

#article file link list and author list
arti_list = c() #a list of all article docs (50 articles x 50 authors)
author_list_te = c() # a list of author names corresponding every doc in the corpus

for (author in author_folder) {
  name = substring(author, first = 59) #slice the folder link to obtain the name of the author
  articles = Sys.glob(paste0(author, '/*.txt')) #a list of all 50 articles from one author
  arti_list = append(arti_list, articles)
  author_list_te = append(author_list_te, rep(name, times = length(articles)))
}

#use the link list to create a list of readPlaindocs
c50_te = lapply(arti_list, readerPlain)

#clean the doc names and apply them to c50_te
mynames = arti_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
names(c50_te) = mynames

#create the test corpus
cps_te_raw = Corpus(VectorSource(c50_te))
```

### Tokenization on test corpus
```{r}
cps_te = cps_te_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace)) %>%      # remove excess white-space
  tm_map(content_transformer(removeWords), 
         stopwords("en"))                               #remoce stop words

cps_te
```

### create the DTM and TF-IDF matrix for test corpus

```{r, results="hide"}
#the DTM matrix
DTM_te = DocumentTermMatrix(cps_te, control = list(dictionary = colnames(DTM_tr))) 
#This ensure the DTM_te has the same col structure (the terms) as DTM_tr

#the TF-IDF matrix
tfidf_te = as.matrix(weightTfIdf(DTM_te))
```


# PCA on TF-IDF Matrix

*We first did some clean-up on train and test matrices to make sure they don't have empty columns and have the same size. Then we run PCA on the train TF-IDF matrix.*

## clean up empty columns in TF-IDF matrices
```{r}
#filter out empty columns
tfidf_tr_1 = tfidf_tr[,which(colSums(tfidf_tr) != 0)] 
tfidf_te_1 = tfidf_te[,which(colSums(tfidf_te) != 0)] 
#make sure the train and test matrices have the same size
tfidf_te_1 = tfidf_te_1[,intersect(colnames(tfidf_te_1),colnames(tfidf_tr_1))]
tfidf_tr_1 = tfidf_tr_1[,intersect(colnames(tfidf_te_1),colnames(tfidf_tr_1))]
```

## Set up PCA for train X
```{r}
c50_pca = prcomp(tfidf_tr_1, scale = TRUE) 
```

## Plot the cumulative explained variance(CEV) by PCA components
```{r}
cum_v = cumsum(c50_pca$sdev^2 / sum(c50_pca$sdev^2))
plot(cum_v, xlab = "PCA Compoents", 
     ylab = "CEV", 
     main = "cumulative explained variance (CEV) plot")
abline(v = 900, col="blue", lty=10)
abline(h = .82, col="blue", lty=10)
points(x = 900, y = .82, col="red", lty=5, pch = 19)
legend("bottomright", 
       legend=c("Cut-off at PCA Comp #900"),
       col=c("blue"), lty=10, cex=0.6)
```
*We found that CEV reaches somewhere above 80% (which is good enough) at the 900th of the PCA components, thus we will use these 900 PCA comps to train our models.*

# Model Comparison in Author Attribution

*We used Randomforest and KNN models to do author attribution on the test data and compared their performance.*

## Set up train and test data (author ~ pca comps)
```{r}
#using the first 400 pca comps
#train data
c50_tr = data.frame(c50_pca$x[,1:900])
c50_tr['author'] = as.factor(author_list_tr)

#test data
c50_load = c50_pca$rotation[, 1:900]
c50_te_pre = scale(tfidf_te_1) %*% c50_load
c50_te = as.data.frame(c50_te_pre)
c50_te['author'] = as.factor(author_list_tr)
```


## RandomForest

*We did a 5-fold CV in order to find out the optimal mty for the RF model. Then we did author attribution using the optimal RF model.*

## 5-fold CV for optimal mty
```{r}
#this chunk will cost about 10min to run
#Do a 10-fold CV with 3 repeats in each to pick up optimal tree numbers (B values)
control = trainControl(method="repeatedcv", number=5, repeats=3)

#set up arguments for train()
metric =  "Accuracy"
mtry = sqrt(ncol(c50_tr) - 1)
tunegrid = expand.grid(.mtry=mtry)

#model fit
model_rf = train(author ~ .,
                 data = c50_tr, 
                 method="rf", 
                 metric = metric, 
                 tuneGrid = tunegrid, 
                 trControl = control)
#optimal mty = 30
```

### Calculate accuracy rate on test data
```{r}
#prediction on test data
c50_rf_te_pred = predict(model_rf,
                         newdata = c50_te)
#confusion matrix
conf_rf = confusionMatrix(c50_rf_te_pred, data = c50_te$author)
c50_rf_accuracy = conf_rf$overall[1]

cat('Random Forest Model gives us an accruacy rate of', c50_rf_accuracy)
```


## KNN

*We did a 5-fold CV in order to find out the optimal k for the KNN model. Then we did author attribution using the optimal KNN model.*

#5-fold CV to pick up the optimal k
```{r}
#this chunk will cost about 5min to run
#set up a 5-fold cv 
tr_control = trainControl(method  = "cv",
                          number  = 5)

#Test knn models with k = 1-10
c50_knn = train(author ~ .,
                method = "knn",
                tuneGrid = expand.grid(k = 1:10),
                trControl = tr_control,
                metric = "Accuracy",
                data = c50_tr)
#optimal k=1
```

### Plot accruacy level for k = 1-10
```{r}

k_accuracy = c50_knn$results[, c(1, 2)]
plot(k_accuracy, xlab = "k", 
     ylab = "Accuracy", 
     main = "5-fold CV for KNN")
points(x = 1, y = k_accuracy[1, 2], col="red", lty=5, pch = 19)
```
*Since the KNN with k=1 has the hightest accuracy, we pick optimal k = 1.*

### Calculate accuracy rate on test data
```{r}
#prediction on test data
c50_knn_te_pred = predict(c50_knn,
                          newdata = c50_te)

#compare prediction with actual values
compare_knn = as.data.frame(cbind(actual = c50_te$author, 
                                  pred = c50_knn_te_pred))
compare_knn['hit'] = ifelse(compare_knn$actual == compare_knn$pred, 1, 0)
c50_knn_accuracy = sum(compare_knn$hit) / nrow(compare_knn)
cat('KNN Model gives us an accruacy rate of', c50_knn_accuracy)
```


# Conclusion

*Among the two models we used (RandomForest and KNN), RandomForest model has the best accuracy of 57% while KNN gives an accuracy of 32%.*

*There are potential ways of improvement to our models' performance. First we could implement more PCA components when fitting the models in order to capture more data variance. Second in our case we ignored the words in test data which don't show in the train data. We will figure out a way to include these words in the future to improve our model performance. Lastly we will try other classification models like logistic regression in doing author attribution in the future.*






